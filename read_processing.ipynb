{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"28741\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"28741\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"28741\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '28741' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"28741\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"28741\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"28741\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '28741' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"28741\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from subprocess import Popen\n",
    "from time import sleep, time\n",
    "import signal\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook, push_notebook\n",
    "from bokeh.models import ColumnDataSource, CDSView\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import brewer\n",
    "from collections import OrderedDict\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths/parameters\n",
    "fastq_dir = \"/home/anthony/siRNA_profiling/FASTQ/\"\n",
    "output_dir = \"/home/anthony/profiling_pipeline/sample_data/\"\n",
    "available_cores = 96 #TODO:deal with situation if threads are fewer than number of samples.\n",
    "star_ncRNA_dir = \"/home/anthony/reference_genome/boris_profiling_annotations/hg38_Anno_BZ/ncRNA_STAR\"\n",
    "star_genome_dir = \"/home/anthony/reference_genome/STAR_index/\"\n",
    "star_reporter_dir = \"/home/anthony/siRNA_profiling/reporter_fasta/reporter_STAR_index/\" # if no reporter, set to False\n",
    "\n",
    "fastq_dir += \"/\" if not fastq_dir.endswith(\"/\") else \"\"\n",
    "output_dir += \"/\" if not output_dir.endswith(\"/\") else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename fasta files\n",
    "barcode_name_mapping = {\n",
    "    \"GCCAAT\":\"ASCC3\",\n",
    "    \"CAGATC\":\"ASCC2-TRIP4\",\n",
    "    \"ACTTGA\":\"ASCC3-ASCC2-TRIP4-ZNF598\",\n",
    "    \"GATCAG\":\"Scrambled\",\n",
    "    \"GGCTAC\":\"Mock\",\n",
    "}\n",
    "\n",
    "files = os.listdir(fastq_dir)\n",
    "for file in files:\n",
    "    match = re.search(\"(.+)(\\.fastq|\\.fastq\\.gz)$\", file)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        suffix = match.group(2)\n",
    "        for barcode in barcode_name_mapping:\n",
    "            if barcode in prefix:\n",
    "                os.rename(fastq_dir+\"/\"+file, fastq_dir+\"/\"+barcode_name_mapping[barcode]+suffix)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process files:\n",
      "\t\tASCC2-TRIP4.fastq.gz\n",
      "\t\tASCC3-ASCC2-TRIP4-ZNF598.fastq.gz\n",
      "\t\tMock.fastq.gz\n",
      "\t\tScrambled.fastq.gz\n",
      "\t\tASCC3.fastq.gz\n",
      "\n",
      "Using 19/96 cores for each sample.\n",
      "Data will be output to /home/anthony/profiling_pipeline/sample_data/output/\n"
     ]
    }
   ],
   "source": [
    "#set up folder structure for output\n",
    "if os.access(output_dir, os.F_OK):\n",
    "    if not os.access(output_dir+\"output\", os.F_OK):\n",
    "        os.chdir(output_dir)\n",
    "        os.mkdir(\"output\")\n",
    "        os.mkdir(output_dir+\"/output/logs\")\n",
    "        os.mkdir(output_dir+\"/output/logs/pipeline_completion\")\n",
    "        os.mkdir(output_dir+\"/output/deduplicated\")\n",
    "        os.mkdir(output_dir+\"/output/trimmed\")\n",
    "        os.mkdir(output_dir+\"/output/ncRNA_aligned\")\n",
    "        if star_reporter_dir:\n",
    "            os.mkdir(output_dir+\"/output/reporter_aligned\")\n",
    "        os.mkdir(output_dir+\"/output/genome_aligned\")\n",
    "    output_dir += \"output/\"\n",
    "else:\n",
    "    print(\"WARNING:\", output_dir, \"does not exist.\")\n",
    "if not os.access(output_dir, os.F_OK):\n",
    "    print(\"WARNING:\", fastq_dir, \"does not exist.\")\n",
    "\n",
    "#check file endings for correct data processing.\n",
    "filenames = os.listdir(fastq_dir)\n",
    "fastq_files = []\n",
    "for filename in filenames:\n",
    "    if filename.endswith(\".fastq\") or filename.endswith(\".fastq.gz\"):\n",
    "        fastq_files.append(filename)\n",
    "\n",
    "#calculate the number of cores by dividing available cores by number of samples.\n",
    "numcores = max(available_cores//len(fastq_files), 1)\n",
    "\n",
    "print(\"Ready to process files:\")\n",
    "[print(\"\\t\\t\"+file) for file in fastq_files]\n",
    "print(\"\\nUsing\", str(numcores)+\"/\"+str(available_cores), \"cores for each sample.\")\n",
    "print(\"Data will be output to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines process handler for running one sample through the pipeline\n",
    "class run_one_sample():\n",
    "    '''handles the running of one sample through the entire pipeline. \n",
    "    This class should be called by an individual thread. It will output stdout/stderr \n",
    "    readouts from the individual steps in the .../output/logs/ folder.\n",
    "    \n",
    "    It also uses a simple text log to pick up where the sample left off if the pipeline \n",
    "    fails. This log is stored in .../output/logs/pipeline_completion/\n",
    "    Steps are defined in self.steps\n",
    "    \n",
    "    The self.check method will terminate the run and kill all running process groups if\n",
    "    one of the processes in this thread returns an exit code other than 0, or if any\n",
    "    other thread changes the global kill_pipeline variable to True. Default behavior is \n",
    "    to set kill_pipeline to True whenever any thread fails, thus halting processing of \n",
    "    all samples.\n",
    "    \n",
    "    On completion, each thread will add one to the global samples_done counter, to \n",
    "    let the main thread know when all are finished.\n",
    "    \n",
    "    TODO:use queues/messages instead of changing global variables\n",
    "    '''\n",
    "    def __init__(self, filename):\n",
    "        global kill_pipeline\n",
    "        global samples_done\n",
    "        global star_ncRNA_dir\n",
    "        global star_genome_dir\n",
    "        global star_reporter_dir\n",
    "        global fastq_dir\n",
    "        global output_dir\n",
    "        global numcores\n",
    "        global failed_sample\n",
    "        global thread_tracker\n",
    "        self.star_ncRNA_dir = star_ncRNA_dir\n",
    "        self.star_genome_dir = star_genome_dir\n",
    "        self.star_reporter_dir = star_reporter_dir\n",
    "        self.fastq_dir = fastq_dir\n",
    "        self.filename = filename\n",
    "        self.output_dir = output_dir\n",
    "        self.numcores = numcores\n",
    "        self.killflag = False\n",
    "        match = re.search(\"(.+)(\\.fastq|\\.fastq\\.gz)$\", self.filename)\n",
    "        self.prefix = match.group(1)\n",
    "        self.suffix = match.group(2)\n",
    "        if self.prefix not in thread_tracker:\n",
    "            thread_tracker[self.prefix] = \"initiate\"\n",
    "        if self.star_reporter_dir:\n",
    "            self.steps = [\"initiate\", \"deduplicate\", \"trim\", \"align_ncRNA\", \"align_reporter\", \"remove_reverse_reads\", \"sort_reporter_aligned\", \n",
    "                          \"index_reporter_aligned\", \"align_genome\", \"sort_genome_aligned\", \"index_genome_aligned\", \"done\"] \n",
    "        else:\n",
    "            self.steps = [\"initiate\", \"deduplicate\", \"trim\", \"align_ncRNA\", \"align_genome\", \n",
    "                          \"sort_genome_aligned\", \"index_genome_aligned\", \"done\"]\n",
    "        if not os.access(output_dir+\"logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", os.F_OK):\n",
    "            self.current_step = 0\n",
    "            with open(output_dir+\"logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", \"w\") as self.completion:\n",
    "                self.completion.write(\"initiate\\n\")\n",
    "        else:\n",
    "            with open(output_dir+\"logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", \"r\") as self.completion:\n",
    "                self.current_step = self.steps.index(self.completion.readlines()[-1].strip())\n",
    "                thread_tracker[self.prefix] = self.steps[self.current_step]\n",
    "        \n",
    "        with open(output_dir+\"/logs/\"+self.prefix+\"_stdout.txt\", \"a\") as self.stdout, \\\n",
    "            open(output_dir+\"/logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", \"a\") as self.completion:\n",
    "            try:\n",
    "                self.proc = self.deduplicate(fastq_dir, output_dir, self.filename, numcores)\n",
    "                self.check(self.proc)\n",
    "                self.proc = self.trim_reads(output_dir, self.filename, numcores)\n",
    "                self.check(self.proc)\n",
    "                self.proc = self.align_ncRNA(output_dir, self.filename, numcores)\n",
    "                self.check(self.proc)\n",
    "                if self.star_reporter_dir:\n",
    "                    self.proc = self.align_reporter(output_dir, self.filename, numcores)\n",
    "                    self.check(self.proc)\n",
    "                    self.proc = self.samtools_remove_rv_reads(\"reporter_aligned\", numcores)\n",
    "                    self.check(self.proc)\n",
    "                    self.proc = self.samtools_sort(\"reporter_aligned\", numcores)\n",
    "                    self.check(self.proc)\n",
    "                    self.proc = self.samtools_index(\"reporter_aligned\", numcores)\n",
    "                    self.check(self.proc)\n",
    "                self.proc = self.align_genome(output_dir, self.filename, numcores)\n",
    "                self.check(self.proc)\n",
    "                self.proc = self.samtools_sort(\"genome_aligned\", numcores)\n",
    "                self.check(self.proc)\n",
    "                self.proc = self.samtools_index(\"genome_aligned\", numcores)\n",
    "                self.check(self.proc)\n",
    "                #thread_tracker[self.prefix] = \"done\"\n",
    "                #self.completion.write(\"done.\\n\")\n",
    "                samples_done += 1\n",
    "            except:\n",
    "                self.killflag = True\n",
    "                kill_pipeline = True\n",
    "                samples_done += 1 \n",
    "                try:\n",
    "                    os.killpg(os.getpgid(self.proc.pid), signal.SIGTERM)\n",
    "                except ProcessLookupError:\n",
    "                    pass\n",
    "                raise\n",
    "    \n",
    "    def deduplicate(self, fastq_dir, output_dir, filename, numcores):\n",
    "        '''deduplicate ribosome profiling reads using dedupe.sh from \n",
    "        the BBTools suite.\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"deduplicate\")-1:\n",
    "            self.stdout.write(\"Deduplicate\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            dedupe_dir = output_dir+\"deduplicated/\"+self.prefix+\"/\"\n",
    "            if not os.access(dedupe_dir, os.F_OK):\n",
    "                os.mkdir(dedupe_dir)\n",
    "            return Popen([\n",
    "                \"dedupe.sh\",\n",
    "                \"in=\"+fastq_dir+filename,\n",
    "                \"out=\"+dedupe_dir+self.prefix+\".deduped\"+self.suffix,\n",
    "                \"absorbmatch=t\", #absorb exact matches of contigs\n",
    "                \"absorbcontainment=f\", #do not absorb full containments of contigs\n",
    "                \"absorbrc=f\", #do not absorb reverse-compliments\n",
    "                \"threads=\"+str(numcores),\n",
    "                \"overwrite=t\",\n",
    "            ], stdout=self.stdout, stderr=self.stdout, preexec_fn=os.setsid)\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    def trim_reads(self, output_dir, filename, numcores):\n",
    "        '''Trim adapters and low quality regions from reads using bbduk.sh\n",
    "        from the BBTools suite.\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"trim\")-1:\n",
    "            self.stdout.write(\"\\n\\nTrim Reads\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            dedupe_dir = output_dir+\"deduplicated/\"+self.prefix+\"/\"\n",
    "            trimmed_dir = output_dir+\"trimmed/\"+self.prefix+\"/\"\n",
    "            if not os.access(trimmed_dir, os.F_OK):\n",
    "                os.mkdir(trimmed_dir)\n",
    "                os.mkdir(trimmed_dir+\"failedQC\")\n",
    "            return Popen([\n",
    "                \"bbduk.sh\",\n",
    "                \"in=\"+dedupe_dir+self.prefix+\".deduped\"+self.suffix,\n",
    "                \"out=\"+trimmed_dir+self.prefix+\".trimmed.fastq\",\n",
    "                \"outm=\"+trimmed_dir+\"failedQC/\"+self.prefix+\".failedQC\"+self.suffix,\n",
    "                \"rpkm=\"+trimmed_dir+\"rpkm.txt\",\n",
    "                \"refstats=\"+trimmed_dir+\"trimming_stats.txt\",\n",
    "                \"literal=NNNNNNCACTCGGGCACCAAGGAC\",\n",
    "                \"k=24\", # this parameter sets the minimum kmer being trimmed. \n",
    "                                      #Longer = more specific, shorter = more sensitive\n",
    "                \"mink=8\", #includes truncations of the kmers down to 8\n",
    "                \"mm=f\", #do not ignore middle base mismatch of kmer\n",
    "                \"rcomp=f\", #do not allow reverse complement kmer matches\n",
    "                \"copyundefined=t\",\n",
    "                \"ktrim=r\",\n",
    "                \"forcetrimleft=4\", #removes random barcode on left of reads.\n",
    "                \"minavgquality=10\",\n",
    "                \"minlength=10\",\n",
    "                \"threads=\"+str(numcores),\n",
    "                \"overwrite=t\",\n",
    "            ],\n",
    "            stdout=self.stdout, stderr=self.stdout, preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def align_ncRNA(self, output_dir, filename, numcores):\n",
    "        '''Align reads to ncRNA using STAR. ncRNA fasta sequences from Boris.\n",
    "        Output unaligned reads.\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"align_ncRNA\")-1:\n",
    "            self.stdout.write(\"\\n\\nAlign to ncRNA\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            trimmed_dir = output_dir+\"trimmed/\"+self.prefix+\"/\"\n",
    "            ncRNA_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "            if not os.access(ncRNA_aligned_dir, os.F_OK):\n",
    "                os.mkdir(ncRNA_aligned_dir)\n",
    "            command = [\n",
    "                \"STAR\",\n",
    "                \"--runThreadN\", str(numcores),\n",
    "                \"--genomeDir\", self.star_ncRNA_dir,\n",
    "                \"--readFilesIn\", trimmed_dir+self.prefix+\".trimmed.fastq\",\n",
    "                \"--outFileNamePrefix\", ncRNA_aligned_dir+self.prefix+\"_\",\n",
    "                \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "                \"--outReadsUnmapped\", \"Fastx\",\n",
    "                \"--alignSJDBoverhangMin\", \"1\",\n",
    "                \"--alignSJoverhangMin\", \"8\",\n",
    "                \"--outFilterMultimapNmax\", \"20\",\n",
    "                \"--outFilterType\", \"BySJout\",\n",
    "            ]\n",
    "            return Popen(command, stderr=self.stdout, stdout=self.stdout, \n",
    "                         preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def align_reporter(self, output_dir, filename, numcores):\n",
    "        '''Align reads to reporter sequence using STAR.\n",
    "        Output unaligned reads.\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"align_reporter\")-1:\n",
    "            self.stdout.write(\"\\n\\nAlign to reporter\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            ncRNA_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "            reporter_aligned_dir = output_dir+\"reporter_aligned/\"+self.prefix+\"/\"\n",
    "            if not os.access(reporter_aligned_dir, os.F_OK):\n",
    "                os.mkdir(reporter_aligned_dir)\n",
    "            command = [\n",
    "                \"STAR\",\n",
    "                \"--runThreadN\", str(numcores),\n",
    "                \"--genomeDir\", self.star_reporter_dir,\n",
    "                \"--readFilesIn\", ncRNA_aligned_dir+self.prefix+\"_Unmapped.out.mate1\",\n",
    "                \"--outFileNamePrefix\", reporter_aligned_dir+self.prefix+\"_\",\n",
    "                \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "                \"--outReadsUnmapped\", \"Fastx\",\n",
    "                \"--alignSJDBoverhangMin\", \"1\",\n",
    "                \"--alignSJoverhangMin\", \"8\",\n",
    "                \"--outFilterMultimapNmax\", \"1\",\n",
    "                \"--outFilterType\", \"BySJout\",\n",
    "            ]\n",
    "            return Popen(command, stderr=self.stdout, stdout=self.stdout, \n",
    "                         preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def align_genome(self, output_dir, filename, numcores):\n",
    "        '''Align remaining reads to genome.\n",
    "        '''\n",
    "        if self.killflag == False and (self.current_step == self.steps.index(\"align_genome\")-1):\n",
    "            self.stdout.write(\"\\n\\nAlign to genome\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            if self.star_reporter_dir:\n",
    "                previous_aligned_dir = output_dir+\"reporter_aligned/\"+self.prefix+\"/\"\n",
    "            else:\n",
    "                previous_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "            tx_aligned_dir = output_dir+\"genome_aligned/\"+self.prefix+\"/\"\n",
    "            if not os.access(tx_aligned_dir, os.F_OK):\n",
    "                os.mkdir(tx_aligned_dir)\n",
    "            command = [\n",
    "                \"STAR\",\n",
    "                \"--runThreadN\", str(self.numcores),\n",
    "                \"--genomeDir\", self.star_genome_dir,\n",
    "                \"--readFilesIn\", previous_aligned_dir+self.prefix+\"_Unmapped.out.mate1\",\n",
    "                \"--outFileNamePrefix\", tx_aligned_dir+self.prefix+\"_\",\n",
    "                \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "                \"--outReadsUnmapped\", \"Fastx\",\n",
    "                \"--alignSJDBoverhangMin\", \"1\",\n",
    "                \"--alignSJoverhangMin\", \"8\",\n",
    "                \"--outFilterMultimapNmax\", \"200\", #how many multimap sites allowed for read\n",
    "                \"--outSAMmultNmax\", \"1\", #how many map sites to write to output for each read\n",
    "                \"--outMultimapperOrder\", \"Random\", #assign read to random alignment if multimapper\n",
    "                \"--outFilterType\", \"BySJout\",\n",
    "            ]\n",
    "            return Popen(command, stderr=self.stdout, stdout=self.stdout,\n",
    "                         preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def samtools_sort(self, input_dir, numcores):\n",
    "        '''Sort BAM file from STAR ouput.\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"sort_\"+input_dir)-1:\n",
    "            self.stdout.write(\"\\n\\nSort \"+input_dir+\" BAM file\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            if input_dir == \"reporter_aligned\":\n",
    "                aligned_suffix = \"_Aligned.out.plusstrand\"\n",
    "            else:\n",
    "                aligned_suffix = \"_Aligned.out\"\n",
    "            return Popen([\n",
    "                \"samtools\",\n",
    "                \"sort\",\n",
    "                \"-@\", str(numcores),\n",
    "                self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".bam\",\n",
    "                \"-o\", self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".sorted.bam\"\n",
    "            ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def samtools_index(self, input_dir, numcores):\n",
    "        '''Index BAM file from STAR output\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"index_\"+input_dir)-1:\n",
    "            self.stdout.write(\"\\n\\nIndex \"+input_dir+\" BAM file\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            if input_dir == \"reporter_aligned\":\n",
    "                aligned_suffix = \"_Aligned.out.plusstrand\"\n",
    "            else:\n",
    "                aligned_suffix = \"_Aligned.out\"\n",
    "            try:\n",
    "                os.remove(self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".bam\",)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            return Popen([\n",
    "                \"samtools\",\n",
    "                \"index\",\n",
    "                \"-@\", str(numcores),\n",
    "                self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".sorted.bam\"\n",
    "            ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def samtools_remove_rv_reads(self, input_dir, numcores):\n",
    "        '''Remove reverse reads aligned to the reporter minus strand with samtools\n",
    "        '''\n",
    "        if self.killflag == False and self.current_step == self.steps.index(\"remove_reverse_reads\")-1:\n",
    "            self.stdout.write(\"\\n\\nRemove reporter reverse strand reads\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            return Popen([\n",
    "                \"samtools\",\n",
    "                \"view\",\n",
    "                \"-@\", str(numcores),\n",
    "                \"-F\", \"0x10\", #only include reads with this flag, which means plus strand alignment\n",
    "                \"-o\",\n",
    "                self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+\"_Aligned.out.plusstrand.bam\",\n",
    "                self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+\"_Aligned.out.bam\"\n",
    "            ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "        \n",
    "        \n",
    "    def check(self, proc):\n",
    "        '''Poll Popen processes returned by each method to determine if a nonzero\n",
    "        error code was returned. If so, kill process group and set global kill_pipeline\n",
    "        to True, signalling to other processing threads to shut down as well.\n",
    "        Polling happens every 1 second.\n",
    "        '''\n",
    "        global kill_pipeline\n",
    "        global thread_tracker\n",
    "        try:\n",
    "            exit_code = proc.poll()\n",
    "            while exit_code == None:\n",
    "                exit_code = proc.poll()\n",
    "                sleep(1)\n",
    "                if kill_pipeline == True:\n",
    "                    self.killflag = True\n",
    "                if self.killflag == True:\n",
    "                    os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
    "            else:\n",
    "                if exit_code != 0:\n",
    "                    self.killflag = True\n",
    "                    kill_pipeline = True\n",
    "                else:\n",
    "                    self.current_step += 1\n",
    "                    self.completion.write(self.steps[self.current_step]+\"\\n\")\n",
    "                    thread_tracker[self.prefix] = self.steps[self.current_step]\n",
    "                    self.completion.flush()\n",
    "        except (AttributeError, ProcessLookupError): \n",
    "            #necessary to catch errors from methods returning 0 if self.killflag = True\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Running...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"320dab05-b116-4902-95e6-273670d7416b\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"a7a21f3b-b4a2-4083-94ed-3bad6a76a3b9\":{\"roots\":{\"references\":[{\"attributes\":{\"source\":{\"id\":\"183362\",\"type\":\"ColumnDataSource\"}},\"id\":\"183368\",\"type\":\"CDSView\"},{\"attributes\":{\"plot\":null,\"text\":\"Pipeline Progress\"},\"id\":\"183342\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"183349\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"fill_color\":{\"field\":\"y\",\"transform\":{\"id\":\"183363\",\"type\":\"CategoricalColorMapper\"}},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"183365\",\"type\":\"HBar\"},{\"attributes\":{},\"id\":\"183358\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"184028\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"183351\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"183343\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"183358\",\"type\":\"CategoricalTicker\"},\"visible\":false},\"id\":\"183360\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\"},\"id\":\"183361\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null,\"factors\":[\"ASCC2-TRIP4\",\"ASCC3-ASCC2-TRIP4-ZNF598\",\"Mock\",\"Scrambled\",\"ASCC3\"]},\"id\":\"183347\",\"type\":\"FactorRange\"},{\"attributes\":{\"factors\":[\"ASCC2-TRIP4\",\"ASCC3-ASCC2-TRIP4-ZNF598\",\"Mock\",\"Scrambled\",\"ASCC3\"],\"palette\":[\"#2b83ba\",\"#abdda4\",\"#ffffbf\",\"#fdae61\",\"#d7191c\"]},\"id\":\"183363\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{},\"id\":\"184027\",\"type\":\"Selection\"},{\"attributes\":{\"data_source\":{\"id\":\"183362\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"183365\",\"type\":\"HBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"183366\",\"type\":\"HBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"183368\",\"type\":\"CDSView\"}},\"id\":\"183367\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"formatter\":{\"id\":\"184025\",\"type\":\"CategoricalTickFormatter\"},\"plot\":{\"id\":\"183343\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"183358\",\"type\":\"CategoricalTicker\"}},\"id\":\"183357\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"callback\":null,\"data\":{\"height\":[0.5,0.5,0.5,0.5,0.5],\"left\":[0,0,0,0,0],\"right\":[\"Start\",\"Start\",\"Start\",\"Start\",\"Start\"],\"y\":[\"ASCC2-TRIP4\",\"ASCC3-ASCC2-TRIP4-ZNF598\",\"Mock\",\"Scrambled\",\"ASCC3\"]},\"selected\":{\"id\":\"184027\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"184028\",\"type\":\"UnionRenderers\"}},\"id\":\"183362\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"184025\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"formatter\":{\"id\":\"184023\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":0.39269908169872414,\"plot\":{\"id\":\"183343\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"183354\",\"type\":\"CategoricalTicker\"}},\"id\":\"183353\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"background_fill_color\":{\"value\":\"lightgray\"},\"below\":[{\"id\":\"183353\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"183357\",\"type\":\"CategoricalAxis\"}],\"plot_height\":200,\"plot_width\":800,\"renderers\":[{\"id\":\"183353\",\"type\":\"CategoricalAxis\"},{\"id\":\"183356\",\"type\":\"Grid\"},{\"id\":\"183357\",\"type\":\"CategoricalAxis\"},{\"id\":\"183360\",\"type\":\"Grid\"},{\"id\":\"183367\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"183342\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"183361\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"183345\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"183349\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"183347\",\"type\":\"FactorRange\"},\"y_scale\":{\"id\":\"183351\",\"type\":\"CategoricalScale\"}},\"id\":\"183343\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"183354\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"183366\",\"type\":\"HBar\"},{\"attributes\":{\"callback\":null,\"factors\":[\"Start\",\"Deduplicating\",\"Trimming Adapter\",\"Aligning to ncRNA\",\"Aligning to Reporter\",\"Filtering reporter reverse reads\",\"Sorting reporter-aligned reads\",\"Indexing reporter-aligned reads\",\"Aligning to Genome\",\"Sorting reporter aligned reads\",\"Indexing Genome Aligned Reads\",\"Done\"]},\"id\":\"183345\",\"type\":\"FactorRange\"},{\"attributes\":{\"grid_line_color\":{\"value\":\"gray\"},\"plot\":{\"id\":\"183343\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"183354\",\"type\":\"CategoricalTicker\"}},\"id\":\"183356\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"184023\",\"type\":\"CategoricalTickFormatter\"}],\"root_ids\":[\"183343\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"a7a21f3b-b4a2-4083-94ed-3bad6a76a3b9\",\"notebook_comms_target\":\"184029\",\"roots\":{\"183343\":\"320dab05-b116-4902-95e6-273670d7416b\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "183343"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline finished successfully!\n",
      "Run time: 10.91 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "kill_pipeline = False\n",
    "samples_done = 0\n",
    "sample_runs = {}\n",
    "thread_tracker = OrderedDict()\n",
    "\n",
    "#start threads for running each sample through the pipeline\n",
    "for filename in fastq_files:\n",
    "    sample_runs[filename] = Thread(target=run_one_sample, args=(filename,))\n",
    "    sample_runs[filename].start()\n",
    "    \n",
    "print(\"Pipeline Running...\")\n",
    "\n",
    "if star_reporter_dir:\n",
    "    steps_mapper = OrderedDict([ #offset by one because if it's recorded in the log, it's already done\n",
    "        (\"initiate\", \"Deduplicating\"), \n",
    "        (\"deduplicate\", \"Trimming Adapter\"),\n",
    "        (\"trim\", \"Aligning to ncRNA\"),\n",
    "        (\"align_ncRNA\", \"Aligning to Reporter\"),\n",
    "        (\"align_reporter\", \"Filtering reporter reverse reads\"),\n",
    "        (\"remove_reverse_reads\", \"Sorting reporter-aligned reads\"),\n",
    "        (\"sort_reporter_aligned\", \"Indexing reporter-aligned reads\"),\n",
    "        (\"index_reporter_aligned\", \"Aligning to Genome\"), \n",
    "        (\"align_genome\", \"Sorting reporter aligned reads\"), \n",
    "        (\"sort_genome_aligned\", \"Indexing Genome Aligned Reads\"), \n",
    "        (\"index_genome_aligned\", \"Done\"),\n",
    "    ])\n",
    "\n",
    "steps = [\"Start\"]+[steps_mapper[step] for step in steps_mapper]\n",
    "prefixes = list(thread_tracker.keys())\n",
    "\n",
    "p = figure(height=200, width=800, y_range=prefixes, background_fill_color=\"lightgray\", x_range=steps, title=\"Pipeline Progress\", tools=[])\n",
    "p.xgrid.grid_line_color = \"gray\"\n",
    "p.xaxis.major_label_orientation = np.pi/8\n",
    "\n",
    "p.ygrid.visible = False\n",
    "source = ColumnDataSource(data={\"y\":prefixes, \"right\":[\"Start\"]*len(prefixes), \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)})\n",
    "x = p.hbar(y=\"y\", right=\"right\", height=\"height\", left=\"left\", color=factor_cmap(field_name=\"y\", palette=brewer[\"Spectral\"][len(prefixes)], factors=prefixes), \n",
    "           line_color=\"black\", source=source)\n",
    "\n",
    "show(p, notebook_handle=True)\n",
    "\n",
    "#Check every 1 sec whether the pipeline is finished and report wether it's been terminated.\n",
    "try:\n",
    "    while samples_done != len(fastq_files):\n",
    "        for sample in thread_tracker:\n",
    "            source.data = {\"y\":prefixes, \"right\":[steps_mapper[thread_tracker[sample]] for sample in thread_tracker], \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)}\n",
    "            x.view = CDSView(source=x.data_source)\n",
    "            push_notebook()\n",
    "        sleep(1)\n",
    "    else:\n",
    "        if kill_pipeline == False:\n",
    "            source.data = {\"y\":prefixes, \"right\":[steps_mapper[thread_tracker[sample]] for sample in thread_tracker], \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)}\n",
    "            x.view = CDSView(source=x.data_source)\n",
    "            push_notebook()\n",
    "            print(\"Pipeline finished successfully!\")\n",
    "        else:\n",
    "            print(\"Run terminated. Check for errors\")\n",
    "except:\n",
    "    kill_pipeline = True  #allows KeyboardInterrupt to kill pipeline\n",
    "    raise\n",
    "\n",
    "    \n",
    "\n",
    "runtime = time() - start_time\n",
    "if runtime > 60:\n",
    "    mins = round(runtime/60, 2)\n",
    "    print(\"Run time:\", mins, \"minutes\")\n",
    "else:\n",
    "    secs = round(runtime, 2)\n",
    "    print(\"Run time:\", secs, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fate(sample_name):\n",
    "    global output_dir\n",
    "    with open(output_dir+\"logs/\"+sample_name+\"_stdout.txt\", \"r\") as log, \\\n",
    "    open(output_dir+\"ncRNA_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as ncRNA_log, \\\n",
    "    open(output_dir+\"reporter_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as reporter_log, \\\n",
    "    open(output_dir+\"genome_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as genome_log:\n",
    "        file = log.readlines()[::-1]\n",
    "        dedupe_end = file.index(\"Trim Reads\\n\")\n",
    "        dedupe_start = file.index(\"--------------------\\n\", dedupe_end)\n",
    "        for line in file[dedupe_end:dedupe_start]:\n",
    "            if line.startswith(\"Input\"):\n",
    "                input_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "            elif line.startswith(\"Result\"):\n",
    "                deduplicated_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "        trim_end = file.index(\"Align to ncRNA\\n\")\n",
    "        trim_start = file.index(\"--------------------\\n\", trim_end)\n",
    "        for line in file[trim_end:trim_start]:\n",
    "            if line.startswith(\"Result\"):\n",
    "                trimmed_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "        file = ncRNA_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                ncRNA_mapped_reads = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                ncRNA_mapped_reads += int(line.split(\"|\")[1].strip())\n",
    "        file = reporter_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                reporter_mapped = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                reporter_mapped += int(line.split(\"|\")[1].strip())\n",
    "        file = genome_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Number of input reads\"):\n",
    "                genome_input_reads = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                genome_mapped = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                genome_mapped += int(line.split(\"|\")[1].strip())\n",
    "        unmapped_reads = genome_input_reads - genome_mapped\n",
    "        print(\"Library was\", str(round(deduplicated_reads/input_reads*100, 2))+\"%\", \"unique.\")\n",
    "        print(\"Of those,\", str(round(trimmed_reads/deduplicated_reads*100, 2))+\"%\", \"survived trimming.\")\n",
    "        print(str(round(ncRNA_mapped_reads/trimmed_reads*100, 2))+\"%\", \"mapped to ncRNA.\")\n",
    "        print(str(round(reporter_mapped/trimmed_reads*100, 2))+\"%\", \"mapped to reporter.\")\n",
    "        print(str(round(genome_mapped/trimmed_reads*100, 2))+\"%\", \"mapped to the genome.\")\n",
    "        print(str(round(unmapped_reads/trimmed_reads*100, 2))+\"%\", \"remained unmapped.\")\n",
    "                \n",
    "for barcode in barcode_name_mapping:\n",
    "    print(barcode_name_mapping[barcode])\n",
    "    read_fate(barcode_name_mapping[barcode])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
